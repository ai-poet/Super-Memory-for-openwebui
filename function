"""
title: è¶…çº§è®°å¿†åŠ©æ‰‹ v4.7
description: 
1.è‡ªåŠ¨åˆ†æå­˜å‚¨è®°å¿†ï¼Œåˆ†ä¸ºfactæ¨¡å¼ï¼ˆå³ç›´æ¥æå–éœ€è¦è®°å¿†çš„äº‹å®å†™å…¥è®°å¿†ï¼‰å’Œsummaryæ¨¡å¼ï¼ˆä»¥aiè‡ªå·±çš„è§†è§’æ€»ç»“è®°å¿†å†™å…¥ï¼‰ã€‚
2.ä¸ºæ¯ä¸€æ¡æ–°å­˜å…¥çš„è®°å¿†ï¼Œéƒ½æ‰“ä¸Šä¸€ä¸ªç²¾ç¡®åˆ°åˆ†é’Ÿçš„æ—¶é—´æˆ³ï¼Œè®©aiçŸ¥é“æ¯æ®µè®°å¿†è®°ä½çš„æ—¶é—´ã€‚
3.å®šæœŸåå°è‡ªåŠ¨ç”Ÿæˆè®°å¿†æ‘˜è¦ï¼Œå°†é›¶ç¢è®°å¿†åˆå¹¶ä¸ºä¸€ä¸ªå®Œæ•´çš„è®°å¿†æ¨¡å—
4.å¯ä»¥è‡ªåŠ¨æ›´æ–°è®°å¿†ï¼Œæ‹¥æœ‰æŸ¥é‡åŠŸèƒ½ï¼Œä¸ä¼šé‡å¤è®°å¿†å·²æœ‰çš„äº‹å®
5.å…¨é¢çš„æ€§èƒ½æ•°æ®ï¼šå®Œæ•´å±•ç¤ºæ¯ä¸€æ¬¡å¯¹è¯çš„é¦–å­—æ—¶é—´æ˜¾ç¤ºã€æ€»è€—æ—¶ã€AIçš„ç”Ÿæˆé€Ÿåº¦ (TPS) å’Œè¾“å‡ºçš„Tokenæ•°
author: å—é£ 
version: 4.7
required_open_webui_version: >= 0.5.0
"""

# ==================== å¯¼å…¥å¿…è¦çš„åº“ ====================
import json
import asyncio
import time
import datetime
import re
import random
from typing import Optional, Callable, Awaitable, Any, List

import pytz
import aiohttp
from fastapi.requests import Request
from pydantic import BaseModel, Field

from open_webui.main import app as webui_app
from open_webui.models.users import Users
from open_webui.routers.memories import (
    add_memory,
    AddMemoryForm,
    query_memory,
    QueryMemoryForm,
    delete_memory_by_id,
)
from open_webui.utils.misc import get_last_assistant_message

# ==================== æç¤ºè¯åº“ ====================
# ... (æç¤ºè¯å†…å®¹ä¸ä¹‹å‰ç‰ˆæœ¬ç›¸åŒï¼Œä¸ºèŠ‚çœç©ºé—´å·²æŠ˜å ) ...
FACT_EXTRACTION_PROMPT = """ä½ æ­£åœ¨å¸®åŠ©ç»´æŠ¤ç”¨æˆ·çš„â€œè®°å¿†â€â€”â€”å°±åƒä¸€ä¸ªä¸ªç‹¬ç«‹çš„â€œæ—¥è®°æ¡ç›®â€ã€‚ä½ å°†æ”¶åˆ°æœ€è¿‘å‡ æ¡å¯¹è¯ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ¤æ–­ç”¨æˆ·çš„ã€æœ€æ–°ä¸€æ¡ã€‘æ¶ˆæ¯ä¸­ï¼Œæœ‰å“ªäº›ç»†èŠ‚å€¼å¾—ä½œä¸ºâ€œè®°å¿†â€è¢«é•¿æœŸä¿å­˜ã€‚ã€æ ¸å¿ƒæŒ‡ä»¤ã€‘1.  **åªåˆ†æç”¨æˆ·æœ€æ–°ä¸€æ¡æ¶ˆæ¯**ï¼šä»…ä»ç”¨æˆ·çš„æœ€æ–°å‘è¨€ä¸­è¯†åˆ«æ–°çš„æˆ–å˜æ›´çš„ä¸ªäººä¿¡æ¯ã€‚æ—§æ¶ˆæ¯ä»…ä¾›ç†è§£ä¸Šä¸‹æ–‡ã€‚2.  **å¤„ç†ä¿¡æ¯å˜æ›´**ï¼šå¦‚æœç”¨æˆ·æœ€æ–°æ¶ˆæ¯ä¸æ—§ä¿¡æ¯å†²çªï¼Œåªæå–æ›´æ–°åçš„ä¿¡æ¯ã€‚3.  **äº‹å®ç‹¬ç«‹**ï¼šæ¯æ¡è®°å¿†éƒ½åº”æ˜¯ç‹¬ç«‹çš„â€œäº‹å®â€ã€‚å¦‚æœä¸€å¥è¯åŒ…å«å¤šä¸ªä¿¡æ¯ç‚¹ï¼Œè¯·æ‹†åˆ†ã€‚4.  **æå–æœ‰ä»·å€¼ä¿¡æ¯**ï¼šç›®æ ‡æ˜¯æ•æ‰ä»»ä½•æœ‰åŠ©äºAIåœ¨æœªæ¥æä¾›æ›´ä¸ªæ€§åŒ–æœåŠ¡çš„ä¿¡æ¯ã€‚5.  **å“åº”æ˜ç¡®æŒ‡ä»¤**ï¼šå¦‚æœç”¨æˆ·æ˜ç¡®è¦æ±‚â€œè®°ä½â€ï¼Œå¿…é¡»æå–è¯¥ä¿¡æ¯ã€‚6.  **å¿½ç•¥çŸ­æœŸä¿¡æ¯**ï¼šä¸è¦è®°å½•ä¸´æ—¶æˆ–æ— æ„ä¹‰çš„ç»†èŠ‚ã€‚7.  **æŒ‡å®šæ ¼å¼è¿”å›**ï¼šå°†ç»“æœä»¥ã€JSONå­—ç¬¦ä¸²æ•°ç»„ã€‘çš„æ ¼å¼è¿”å›ã€‚å¦‚æ— ä¿¡æ¯ï¼Œã€åªã€‘è¿”å›ç©ºæ•°ç»„ï¼ˆ`[]`ï¼‰ã€‚ä¸è¦åŠ ä»»ä½•è§£é‡Šã€‚---### ã€ç¤ºä¾‹ã€‘**ç¤ºä¾‹ 1**_è¾“å…¥å¯¹è¯:_- user: ```æˆ‘çˆ±åƒæ©˜å­```- assistant: ```å¤ªæ£’äº†ï¼```- user: ```å…¶å®æˆ‘è®¨åŒåƒæ©˜å­```_æ­£ç¡®è¾“å‡º:_["ç”¨æˆ·è®¨åŒåƒæ©˜å­"]**ç¤ºä¾‹ 2**_è¾“å…¥å¯¹è¯:_- user: ```æˆ‘æ˜¯ä¸€ååˆçº§æ•°æ®åˆ†æå¸ˆã€‚è¯·è®°ä½æˆ‘çš„é‡è¦æ±‡æŠ¥åœ¨3æœˆ15æ—¥ã€‚```_æ­£ç¡®è¾“å‡º:_["ç”¨æˆ·æ˜¯ä¸€ååˆçº§æ•°æ®åˆ†æå¸ˆ", "ç”¨æˆ·åœ¨3æœˆ15æ—¥æœ‰ä¸€æ¬¡é‡è¦æ±‡æŠ¥"]"""
SUMMARY_CREATION_PROMPT = """ä½ æ˜¯AIåŠ©æ‰‹ï¼Œä»»åŠ¡æ˜¯ä¸ºä¸€æ®µå¯¹è¯ç”Ÿæˆç®€æ´çš„æ‘˜è¦ï¼Œä½œä¸ºé•¿æœŸè®°å¿†ã€‚ã€æŒ‡ä»¤ã€‘1. **è§†è§’**: ä»¥ç¬¬ä¸€äººç§°ï¼ˆâ€œæˆ‘â€æˆ–â€œæˆ‘ä»¬â€ï¼‰æ€»ç»“ã€‚2. **å†…å®¹**: æ•æ‰æ ¸å¿ƒè¦ç‚¹ï¼šç”¨æˆ·æ„å›¾ã€ä½ çš„å…³é”®å›ç­”ã€åŒæ–¹å…±è¯†ã€‚3. **æ ¼å¼**: **å¿…é¡»**æ˜¯å•ä¸€æ®µè½çš„æ–‡æœ¬ã€‚4. **ç›®æ ‡**: å¦‚æœå¯¹è¯æœ‰å®è´¨å†…å®¹ï¼Œåˆ™åˆ›å»ºè®°å¿†ã€‚å¦åˆ™ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ã€‚"""
SUMMARY_UPDATE_PROMPT = """ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æœ€æ–°çš„å¯¹è¯å†…å®¹ï¼Œæ›´æ–°ä¸€æ®µå·²æœ‰çš„è®°å¿†æ‘˜è¦ã€‚ã€æŒ‡ä»¤ã€‘1. ä½ å°†æ”¶åˆ°ã€æ—§çš„æ‘˜è¦ã€‘å’Œã€æ–°çš„å¯¹è¯å†…å®¹ã€‘ã€‚2. å°†æ–°å¯¹è¯ä¸­çš„ã€æ ¸å¿ƒä¿¡æ¯ã€‘æ— ç¼åœ°èå…¥åˆ°æ—§æ‘˜è¦ä¸­ã€‚3. ä¿æŒç¬¬ä¸€äººç§°è§†è§’ã€‚4. **å¿…é¡»**è¿”å›ä¸€ä¸ªæ›´æ–°åçš„ã€å®Œæ•´çš„ã€å•ä¸€æ®µè½çš„æ‘˜è¦æ–‡æœ¬ã€‚ã€æ—§çš„æ‘˜è¦ã€‘:{existing_summary}ã€æ–°çš„å¯¹è¯å†…å®¹ã€‘:{new_conversation}"""
FACT_CONSOLIDATION_PROMPT = """ä½ æ­£åœ¨ç®¡ç†ç”¨æˆ·çš„â€œè®°å¿†â€ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ¸…ç†ä¸€ä¸ªå¯èƒ½åŒ…å«ç›¸å…³ã€é‡å æˆ–å†²çªä¿¡æ¯çš„è®°å¿†åˆ—è¡¨ã€‚**ã€å¤„ç†è§„åˆ™ã€‘**1. ä½ å°†æ”¶åˆ°ä¸€ä¸ªJSONæ ¼å¼çš„è®°å¿†åˆ—è¡¨ï¼Œæ¯æ¡å«â€œfactâ€å’Œâ€œcreated_atâ€æ—¶é—´æˆ³ã€‚2. ç”Ÿæˆä¸€ä¸ªæ¸…ç†åçš„æœ€ç»ˆäº‹å®åˆ—è¡¨ï¼Œç¡®ä¿ï¼š - åªåœ¨è®°å¿†**å®Œå…¨é‡å¤**æˆ–å°±åŒä¸€ä¸»é¢˜**ç›´æ¥å†²çª**æ—¶æ‰åˆå¹¶ã€‚ - å¦‚æœé‡å¤æˆ–å†²çªï¼Œ**åªä¿ç•™`created_at`æœ€æ–°çš„é‚£ä¸€æ¡**ã€‚ - å¦‚æœéƒ¨åˆ†ç›¸ä¼¼ä½†ä¸å†²çªï¼ˆä¾‹å¦‚â€œç”¨æˆ·å–œæ¬¢æ©˜å­â€å’Œâ€œç”¨æˆ·å–œæ¬¢ç†Ÿé€çš„æ©˜å­â€ï¼‰ï¼Œåˆ™**ä¸¤è€…éƒ½ä¿ç•™**ã€‚3. è¿”å›æœ€ç»ˆç»“æœæ—¶ï¼Œä½¿ç”¨ä¸€ä¸ªç®€å•çš„ã€JSONå­—ç¬¦ä¸²æ•°ç»„ã€‘æ ¼å¼ã€‚ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€‚---### ã€ç¤ºä¾‹ã€‘_è¾“å…¥ (JSONæ•°ç»„):_`[{"fact": "ç”¨æˆ·æœ€å–œæ¬¢çš„é¢œè‰²æ˜¯é’è‰²", "created_at": 1635500000}, {"fact": "ç”¨æˆ·æœ€å–œæ¬¢çš„é¢œè‰²æ˜¯çº¢è‰²", "created_at": 1636000000}]`_æ­£ç¡®è¾“å‡º (JSONå­—ç¬¦ä¸²æ•°ç»„):_["ç”¨æˆ·æœ€å–œæ¬¢çš„é¢œè‰²æ˜¯çº¢è‰²"]"""
MEMORY_SUMMARIZATION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªè®°å¿†æ‘˜è¦åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†å…³äºä¸€ä¸ªç”¨æˆ·çš„å¤šæ¡ç›¸å…³ä½†é›¶æ•£çš„è®°å¿†ï¼Œåˆå¹¶æˆä¸€ä¸ªç®€æ´ã€å…¨é¢ã€é«˜è´¨é‡çš„æ‘˜è¦ã€‚**ã€æ ¸å¿ƒæŒ‡ä»¤ã€‘**1.  **æ•´åˆä¿¡æ¯**ï¼šæ•è·æ‰€æœ‰è¾“å…¥è®°å¿†ä¸­çš„å…³é”®ä¿¡æ¯ç‚¹ã€‚2.  **è§£å†³å†²çª**ï¼šå¦‚æœä¿¡æ¯æœ‰çŸ›ç›¾ï¼Œä¼˜å…ˆé‡‡çº³æ—¶é—´æˆ³æœ€æ–°çš„ä¿¡æ¯ã€‚3.  **æ¶ˆé™¤å†—ä½™**ï¼šå»é™¤é‡å¤æˆ–æ— æ„ä¹‰çš„ç»†èŠ‚ã€‚4.  **ä¿æŒç²¾å**ï¼šä¿ç•™ç”¨æˆ·çš„æ ¸å¿ƒåå¥½ã€èº«ä»½ç‰¹å¾ã€é‡è¦ç›®æ ‡å’Œå…³ç³»ã€‚5.  **è‡ªç„¶æµç•…**ï¼šæœ€ç»ˆè¾“å‡ºçš„æ‘˜è¦åº”è¯¥åƒä¸€æ®µè‡ªç„¶è¯­è¨€ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªåˆ—è¡¨ã€‚6.  **æ ¼å¼è¦æ±‚**ï¼šã€å¿…é¡»ã€‘åªè¿”å›ä¸€ä¸ªå•ä¸€æ®µè½çš„æ–‡æœ¬æ‘˜è¦ã€‚ä¸è¦åŠ ä»»ä½•è§£é‡Šæˆ–å¤šä½™çš„æ–‡å­—ã€‚**ã€ç¤ºä¾‹ã€‘**_è¾“å…¥çš„å¤šæ¡è®°å¿†:_- "2024å¹´05æœˆ10æ—¥11ç‚¹ï¼šç”¨æˆ·å–œæ¬¢å–å’–å•¡"- "2024å¹´05æœˆ12æ—¥15ç‚¹ï¼šç”¨æˆ·åå¥½ç¾å¼å’–å•¡"- "2024å¹´05æœˆ20æ—¥09ç‚¹ï¼šç”¨æˆ·æåˆ°ä»–æ¯å¤©æ—©ä¸Šéƒ½è¦å–ä¸€æ¯å’–å•¡æç¥"_æ­£ç¡®çš„è¾“å‡ºæ‘˜è¦:_"ç”¨æˆ·æ˜¯ä¸€ä¸ªå’–å•¡çˆ±å¥½è€…ï¼Œæ¯å¤©æ—©ä¸Šä¹ æƒ¯å–ä¸€æ¯ç¾å¼å’–å•¡æ¥æç¥ã€‚"ç°åœ¨ï¼Œè¯·åˆ†æä»¥ä¸‹çš„ç›¸å…³è®°å¿†ï¼Œå¹¶æä¾›ä¸€ä¸ªç®€æ´ã€é«˜è´¨é‡çš„æ‘˜è¦ã€‚"""


# ==================== ä¸»ç±»å®šä¹‰ ====================
class Filter:
    _embedding_model = None
    _background_tasks = set()

    @classmethod
    def get_embedding_model(cls):
        # ... (æ‡’åŠ è½½æ¨¡å‹çš„ä»£ç ä¿æŒä¸å˜) ...
        if cls._embedding_model is None:
            try:
                print("Attempting to load SentenceTransformer model...")
                from sentence_transformers import SentenceTransformer

                cls._embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
                print("Embedding model loaded successfully.")
            except ImportError:
                print(
                    "ERROR: sentence-transformers or numpy not installed. Please run 'pip install sentence-transformers numpy'"
                )
                return None
            except Exception as e:
                print(f"Error loading embedding model: {e}")
                return None
        return cls._embedding_model

    class Valves(BaseModel):
        # ... (é…ç½®ä¸v4.6ä¸€è‡´) ...
        enabled: bool = Field(default=True, description="ã€æ€»å¼€å…³ã€‘å¯ç”¨/ç¦ç”¨æœ¬æ’ä»¶")
        memory_mode: str = Field(
            default="fact",
            description="è®°å¿†æ¨¡å¼: 'fact' (æå–äº‹å®) æˆ– 'summary' (å¯¹è¯æ€»ç»“)",
        )
        api_url: str = Field(
            default="https://api.openai.com/v1/chat/completions",
            description="APIç«¯ç‚¹åœ°å€ (å¿…é¡»åŒ…å«/v1/chat/completions)",
        )
        api_key: str = Field(default="", description="ã€é‡è¦ã€‘APIå¯†é’¥")
        model: str = Field(default="gpt-4o-mini", description="ç”¨äºè®°å¿†å¤„ç†çš„æ¨¡å‹")
        show_stats: bool = Field(default=True, description="åœ¨çŠ¶æ€æ æ˜¾ç¤ºæ€§èƒ½ä¸è®°å¿†ç»Ÿè®¡")
        messages_to_consider: int = Field(default=6, description="çº³å…¥åˆ†æçš„æ¶ˆæ¯æ•°é‡")
        timezone: str = Field(
            default="Asia/Shanghai", description="ã€æ—¶é—´ä¿®æ­£ã€‘ç”¨äºç”Ÿæˆæ—¶é—´æˆ³çš„æ—¶åŒº"
        )
        enable_semantic_search: bool = Field(
            default=False, description="ã€é«˜çº§åŠŸèƒ½/å¯é€‰ã€‘å¯ç”¨è¯­ä¹‰æœç´¢"
        )
        similarity_threshold: float = Field(
            default=0.8, description="è®°å¿†å»é‡/æ›´æ–°çš„ç›¸ä¼¼åº¦é˜ˆå€¼"
        )
        consolidation_threshold: float = Field(
            default=0.75,
            description="ã€è®°å¿†æ•´åˆã€‘æŸ¥æ‰¾ç›¸å…³æ—§è®°å¿†ä»¥è¿›è¡Œæ•´åˆçš„ç›¸ä¼¼åº¦é˜ˆå€¼ (0-1)",
        )
        enable_background_summarization: bool = Field(
            default=True, description="ã€åå°ä»»åŠ¡ã€‘æ˜¯å¦å¯ç”¨åå°è®°å¿†è‡ªåŠ¨æ‘˜è¦ä¸æ•´åˆ"
        )
        summarization_interval_hours: float = Field(
            default=2.0, description="ã€åå°ä»»åŠ¡ã€‘æ¯éš”å¤šå°‘å°æ—¶æ‰§è¡Œä¸€æ¬¡æ‘˜è¦ä»»åŠ¡"
        )
        summarization_cluster_threshold: float = Field(
            default=0.7, description="ã€åå°ä»»åŠ¡ã€‘åˆ¤æ–­è®°å¿†å±äºåŒä¸€ç°‡çš„ç›¸ä¼¼åº¦é˜ˆå€¼"
        )
        summarization_min_cluster_size: int = Field(
            default=3, description="ã€åå°ä»»åŠ¡ã€‘å½¢æˆä¸€ä¸ªç°‡æ‰€éœ€çš„æœ€å°‘è®°å¿†æ•°é‡"
        )
        summarization_min_memory_age_days: int = Field(
            default=7, description="ã€åå°ä»»åŠ¡ã€‘åªå¯¹å¤šå°‘å¤©ä»¥å‰çš„æ—§è®°å¿†è¿›è¡Œæ‘˜è¦"
        )

    def __init__(self):
        self.valves = self.Valves()
        self.start_time = None
        # æ”¹é€ ä¸€ï¼šé‡æ–°åŠ å…¥ç”¨äºè®¡ç®—é¦–å­—æ—¶é—´çš„å˜é‡
        self.time_to_first_token = None
        self.first_chunk_received = False
        if self.valves.enable_background_summarization:
            task = asyncio.create_task(self._summarize_memories_loop())
            self._background_tasks.add(task)
            task.add_done_callback(self._background_tasks.discard)
            print("Background memory summarization task started.")

    def inlet(self, body: dict, __user__: Optional[dict] = None) -> dict:
        self.start_time = time.time()
        # é‡ç½®é¦–å­—æ—¶é—´çŠ¶æ€
        self.time_to_first_token = None
        self.first_chunk_received = False
        return body

    # æ”¹é€ ä¸€(ç»­)ï¼šé‡æ–°åŠ å…¥ stream å‡½æ•°
    def stream(self, event: dict) -> dict:
        if not self.first_chunk_received:
            self.time_to_first_token = time.time() - self.start_time
            self.first_chunk_received = True
        return event

    async def outlet(
        self,
        body: dict,
        __event_emitter__: Callable[[Any], Awaitable[None]],
        __user__: Optional[dict] = None,
    ):
        # ... (outletå‡½æ•°ä¿æŒä¸å˜) ...
        if not self.valves.enabled or not __user__ or len(body.get("messages", [])) < 2:
            return body
        user = Users.get_user_by_id(__user__["id"])
        try:
            if self.valves.memory_mode == "fact":
                memory_result = await self._handle_fact_mode(body, user)
            elif self.valves.memory_mode == "summary":
                memory_result = await self._handle_summary_mode(body, user)
            else:
                memory_result = {"status": "error", "message": "æ— æ•ˆçš„è®°å¿†æ¨¡å¼"}
        except Exception as e:
            print(f"CRITICAL ERROR in outlet: {e}")
            import traceback

            traceback.print_exc()
            memory_result = {"status": "error", "message": "æ’ä»¶æ‰§è¡Œå‡ºé”™"}
        stats_result = self._calculate_stats(body)
        if self.valves.show_stats:
            await self._show_status(__event_emitter__, memory_result, stats_result)
        return body

    # ... (æ‰€æœ‰åå°ä»»åŠ¡ã€è®°å¿†å¤„ç†ç­‰æ ¸å¿ƒé€»è¾‘å‡½æ•°ä¸ v4.6 å®Œå…¨ä¸€è‡´, æ­¤å¤„çœç•¥) ...
    # ä¸ºäº†å®Œæ•´æ€§ï¼Œç²˜è´´æ‰€æœ‰å‡½æ•°
    async def _summarize_memories_loop(self):
        await asyncio.sleep(300)
        while True:
            interval_seconds = self.valves.summarization_interval_hours * 3600
            print(
                f"Next background summarization run in {self.valves.summarization_interval_hours} hours."
            )
            await asyncio.sleep(interval_seconds)
            if not self.valves.enable_semantic_search:
                print("Skipping summarization: Semantic search is disabled in valves.")
                continue
            try:
                all_users = Users.get_users()
                for user in all_users:
                    await self._process_user_summarization(user)
            except Exception as e:
                print(f"Error in background summarization loop: {e}")

    async def _process_user_summarization(self, user):
        print(f"Checking memories for user {user.id}...")
        all_user_memories = await self._query_memories_by_similarity(
            " ", user, k=1000, threshold=0.0
        )
        min_age_seconds = self.valves.summarization_min_memory_age_days * 24 * 3600
        now_ts = time.time()
        eligible_memories = [
            mem
            for mem in all_user_memories
            if (now_ts - self._parse_memory_content(mem["content"])[1])
            > min_age_seconds
        ]
        if len(eligible_memories) < self.valves.summarization_min_cluster_size:
            print(f"User {user.id} has no eligible old memories to summarize.")
            return
        clusters, processed_ids = [], set()
        for mem in eligible_memories:
            if mem["id"] in processed_ids:
                continue
            current_cluster = [mem]
            processed_ids.add(mem["id"])
            related_mems = await self._query_memories_by_similarity(
                mem["content_without_timestamp"],
                user,
                k=10,
                threshold=self.valves.summarization_cluster_threshold,
            )
            for related in related_mems:
                if related["id"] not in processed_ids and related["id"] in [
                    m["id"] for m in eligible_memories
                ]:
                    current_cluster.append(related)
                    processed_ids.add(related["id"])
            if len(current_cluster) >= self.valves.summarization_min_cluster_size:
                clusters.append(current_cluster)
        if not clusters:
            print(f"No memory clusters found for user {user.id}.")
            return
        print(f"Found {len(clusters)} clusters to summarize for user {user.id}.")
        for cluster in clusters:
            try:
                cluster_content = "\n".join([f"- {m['content']}" for m in cluster])
                summary_content = await self._call_llm(
                    MEMORY_SUMMARIZATION_PROMPT, cluster_content
                )
                if summary_content:
                    await self._store_new_memory(summary_content, user, is_summary=True)
                    for m in cluster:
                        await delete_memory_by_id(m["id"], user)
                    print(
                        f"Successfully summarized {len(cluster)} memories into one for user {user.id}."
                    )
            except Exception as e:
                print(f"Error summarizing a cluster for user {user.id}: {e}")

    async def _handle_fact_mode(self, body: dict, user) -> dict:
        conversation_text = self._stringify_conversation(body["messages"])
        if not conversation_text:
            return {"status": "skipped", "message": "æ— æ¶ˆæ¯"}
        try:
            new_facts = await self._call_llm_for_json(
                FACT_EXTRACTION_PROMPT, conversation_text
            )
            if not new_facts:
                return {"status": "success", "message": "æ— æ–°äº‹å®"}
            facts_to_consolidate = []
            related_memories_to_delete_ids = set()
            for fact in new_facts:
                related_memories = await self._query_memories_by_similarity(
                    fact, user, k=5, threshold=self.valves.consolidation_threshold
                )
                consolidation_input = [{"fact": fact, "created_at": time.time()}]
                for mem in related_memories:
                    related_memories_to_delete_ids.add(mem["id"])
                    content, ts = self._parse_memory_content(mem["content"])
                    consolidation_input.append({"fact": content, "created_at": ts})
                facts_to_consolidate.append(consolidation_input)
            final_facts_to_save = []
            for fact_group in facts_to_consolidate:
                prompt_input_json = json.dumps(fact_group, ensure_ascii=False)
                cleaned_facts = await self._call_llm_for_json(
                    FACT_CONSOLIDATION_PROMPT, prompt_input_json
                )
                final_facts_to_save.extend(cleaned_facts)
            if related_memories_to_delete_ids:
                print(
                    f"Consolidating memories, deleting {len(related_memories_to_delete_ids)} old facts..."
                )
                for mem_id in related_memories_to_delete_ids:
                    await delete_memory_by_id(mem_id, user)
            saved_count = 0
            for fact in list(set(final_facts_to_save)):
                await self._store_new_memory(fact, user, is_summary=True)
                saved_count += 1
            return {
                "status": "success",
                "message": f"è®°å¿†æ•´åˆ: æ–°å¢{saved_count}æ¡, æ¸…ç†{len(related_memories_to_delete_ids)}æ¡",
            }
        except Exception as e:
            import traceback

            traceback.print_exc()
            return {"status": "error", "message": f"äº‹å®æ•´åˆå¤±è´¥: {e}"}

    async def _handle_summary_mode(self, body: dict, user) -> dict:
        conversation_text = self._stringify_conversation(body["messages"])
        if not conversation_text:
            return {"status": "skipped", "message": "æ— æ¶ˆæ¯"}
        try:
            related_summary = await self._find_related_summary(conversation_text, user)
            if related_summary:
                updated_summary_content = await self._call_llm(
                    SUMMARY_UPDATE_PROMPT.format(
                        existing_summary=related_summary["content_without_timestamp"],
                        new_conversation=conversation_text,
                    )
                )
                if not updated_summary_content:
                    return {"status": "skipped", "message": "æ›´æ–°æ— å†…å®¹"}
                await delete_memory_by_id(related_summary["id"], user)
                content_with_timestamp = self._add_timestamp_to_content(
                    updated_summary_content
                )
                await add_memory(
                    request=self._get_dummy_request(),
                    form_data=AddMemoryForm(content=content_with_timestamp),
                    user=user,
                )
                return {"status": "success", "message": "æ€»ç»“å·²æ›´æ–°"}
            else:
                new_summary_content = await self._call_llm(
                    SUMMARY_CREATION_PROMPT, conversation_text
                )
                if not new_summary_content:
                    return {"status": "success", "message": "æ— éœ€æ€»ç»“"}
                await self._store_new_memory(new_summary_content, user, is_summary=True)
                return {"status": "success", "message": "å·²åˆ›å»ºæ–°æ€»ç»“"}
        except Exception as e:
            return {"status": "error", "message": f"æ€»ç»“å¤„ç†å¤±è´¥: {e}"}

    # æ”¹é€ ä¸‰ï¼šå°†æ—¶é—´æˆ³æ ¼å¼ç²¾ç¡®åˆ°åˆ†é’Ÿ
    def _add_timestamp_to_content(self, content: str) -> str:
        try:
            target_tz = pytz.timezone(self.valves.timezone)
        except pytz.UnknownTimeZoneError:
            target_tz = pytz.utc
        now = datetime.datetime.now(target_tz)
        return f"{now.strftime('%Yå¹´%mæœˆ%dæ—¥%Hç‚¹%Måˆ†')}ï¼š{content}"

    async def _store_new_memory(
        self, content: str, user, is_summary: bool = False
    ) -> bool:
        if not is_summary:
            is_duplicate = False
            if self.valves.enable_semantic_search:
                related_memories = await self._query_memories_by_similarity(
                    content, user, k=1, threshold=self.valves.similarity_threshold
                )
                if related_memories:
                    is_duplicate = True
            else:
                related_memories = await self._query_memories_legacy(content, user, k=1)
                if (
                    related_memories
                    and related_memories[0]["similarity"]
                    > self.valves.similarity_threshold
                ):
                    is_duplicate = True
            if is_duplicate:
                return False
        content_with_timestamp = self._add_timestamp_to_content(content)
        await add_memory(
            request=self._get_dummy_request(),
            form_data=AddMemoryForm(content=content_with_timestamp),
            user=user,
        )
        return True

    # æ”¹é€ ä¸‰(ç»­)ï¼šä¿®æ­£è§£æå’Œå‰¥ç¦»æ—¶é—´æˆ³çš„é€»è¾‘
    def _parse_memory_content(self, content: str) -> (str, float):
        match = re.match(r"^(\d{4}å¹´\d{2}æœˆ\d{2}æ—¥\d{2}ç‚¹\d{2}åˆ†)ï¼š(.*)", content)
        if match:
            try:
                dt_obj = datetime.datetime.strptime(
                    match.group(1), "%Yå¹´%mæœˆ%dæ—¥%Hç‚¹%Måˆ†ï¼š"
                )
                return match.group(2), dt_obj.timestamp()
            except ValueError:
                return match.group(2), 0
        return content, 0

    async def _find_related_summary(self, text: str, user) -> Optional[dict]:
        related_memories = await self._query_memories_by_similarity(
            text, user, k=1, threshold=self.valves.similarity_threshold
        )
        if not related_memories:
            return None
        return related_memories[0]

    async def _query_memories_legacy(self, text: str, user, k: int = 1) -> List[dict]:
        query_result = await query_memory(
            self._get_dummy_request(), QueryMemoryForm(content=text, k=k), user
        )
        if (
            not query_result
            or not hasattr(query_result, "documents")
            or not query_result.documents[0]
        ):
            return []
        results = []
        for i, doc_content in enumerate(query_result.documents[0]):
            content_without_timestamp, _ = self._parse_memory_content(doc_content)
            results.append(
                {
                    "id": query_result.metadatas[0][i].get("id"),
                    "content": doc_content,
                    "content_without_timestamp": content_without_timestamp,
                    "similarity": 1 - query_result.distances[0][i],
                }
            )
        return results

    async def _query_memories_by_similarity(
        self, text: str, user, k: int, threshold: float
    ) -> List[dict]:
        if self.valves.enable_semantic_search:
            return await self._query_memories_semantic(text, user, k, threshold)
        else:
            results = await self._query_memories_legacy(text, user, k)
            return [res for res in results if res["similarity"] > threshold]

    async def _query_memories_semantic(
        self, text: str, user, k: int = 1, threshold: float = 0.0
    ) -> List[dict]:
        model = self.get_embedding_model()
        if not model:
            return []
        import numpy as np

        all_memories_raw = await self._query_memories_legacy(" ", user, k=1000)
        if not all_memories_raw:
            return []
        query_embedding = model.encode(text)
        results = []
        for mem in all_memories_raw:
            mem_embedding = model.encode(mem["content_without_timestamp"])
            similarity = np.dot(query_embedding, mem_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(mem_embedding)
            )
            if similarity >= threshold:
                mem["similarity"] = float(similarity)
                results.append(mem)
        results.sort(key=lambda x: x["similarity"], reverse=True)
        return results[:k]

    async def _call_llm(self, system_prompt: str, user_prompt: str = "") -> str:
        messages = [{"role": "system", "content": system_prompt}]
        if user_prompt:
            messages.append({"role": "user", "content": user_prompt})
        url = self.valves.api_url
        headers = {
            "Authorization": f"Bearer {self.valves.api_key}",
            "Content-Type": "application/json",
        }
        payload = {"model": self.valves.model, "messages": messages, "temperature": 0.0}
        async with aiohttp.ClientSession() as session:
            async with session.post(url, headers=headers, json=payload) as response:
                response.raise_for_status()
                data = await response.json()
                return data["choices"][0]["message"]["content"].strip()

    async def _call_llm_for_json(
        self, system_prompt: str, user_prompt: str = ""
    ) -> List:
        content = await self._call_llm(system_prompt, user_prompt)
        try:
            if content.startswith("```json"):
                content = content[7:-3].strip()
            result = json.loads(content)
            return result if isinstance(result, list) else []
        except json.JSONDecodeError:
            return []

    def _stringify_conversation(self, messages: List[dict]) -> str:
        count = min(self.valves.messages_to_consider, len(messages))
        return "\n".join(
            [f"- {msg['role']}: {msg['content']}" for msg in messages[-count:]]
        )

    # æ”¹é€ ä¸€(ç»­)ï¼šä¿®æ”¹ç»Ÿè®¡å‡½æ•°ï¼ŒåŠ å…¥é¦–å­—æ—¶é—´
    def _calculate_stats(self, body: dict) -> dict:
        elapsed = time.time() - self.start_time
        response_msg = get_last_assistant_message(body.get("messages", [])) or ""
        tokens = len(response_msg) // 3
        tps = tokens / elapsed if elapsed > 0 else 0
        return {
            "elapsed": f"{elapsed:.1f}s",
            "tokens": tokens,
            "tps": f"{tps:.0f}",
            "ttft": (
                f"{self.time_to_first_token:.2f}s"
                if self.time_to_first_token is not None
                else "N/A"
            ),
        }

    # æ”¹é€ äºŒï¼šä¿®æ”¹æ˜¾ç¤ºå‡½æ•°ï¼Œè°ƒæ•´é¡ºåºå¹¶å¢åŠ é¦–å­—æ—¶é—´
    async def _show_status(
        self, event_emitter, memory_result: dict, stats_result: dict
    ):
        memory_part = []
        if memory_result:
            status, message = memory_result.get("status", "skipped"), memory_result.get(
                "message", ""
            )
            icon = "âœ…" if status == "success" else "âŒ" if status == "error" else "ğŸ¤·"
            memory_part.append(f" {icon} {message}")

        stats_parts = [
            f"é¦–å­—{stats_result['ttft']}",
            f"ç”¨æ—¶{stats_result['elapsed']}",
            f"âš¡{stats_result['tps']}Tok/s",
            f"çº¦{stats_result['tokens']}T",
        ]

        # æŒ‰ç…§æ‚¨è¦çš„é¡ºåºç»„åˆ
        final_parts = memory_part + stats_parts

        if final_parts:
            await event_emitter(
                {
                    "type": "status",
                    "data": {"description": " | ".join(final_parts), "done": True},
                }
            )

    def _get_dummy_request(self) -> Request:
        return Request(scope={"type": "http", "app": webui_app})
