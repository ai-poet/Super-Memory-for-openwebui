"""
title: è¶…çº§è®°å¿†åŠ©æ‰‹ v5.1 (å¸¦è®°å¿†æ³¨å…¥)
description:
1. è‡ªåŠ¨åˆ†æå­˜å‚¨è®°å¿†ï¼Œåˆ†ä¸ºfactæ¨¡å¼å’Œsummaryæ¨¡å¼
2. ä¸ºæ¯æ¡è®°å¿†æ‰“ä¸Šç²¾ç¡®åˆ°åˆ†é’Ÿçš„æ—¶é—´æˆ³
3. è‡ªåŠ¨æ›´æ–°è®°å¿†ï¼Œæ‹¥æœ‰æŸ¥é‡åŠŸèƒ½
4. å…¨é¢çš„æ€§èƒ½æ•°æ®å±•ç¤º
5. å°†ç”¨æˆ·è®°å¿†æ³¨å…¥åˆ°ç³»ç»Ÿæç¤ºè¯ä¸­
6. preserve_min_memory_age_dayså‚æ•°ï¼Œè‡ªåŠ¨æ€»ç»“å¹¶æ¸…ç†æ—§è®°å¿†
version: 5.1
required_open_webui_version: >= 0.5.0
"""

# ==================== å¯¼å…¥å¿…è¦çš„åº“ ====================
import json
import asyncio
import time
import datetime
import re
from typing import Optional, Callable, Awaitable, Any, List

import pytz
import aiohttp
from fastapi.requests import Request
from pydantic import BaseModel, Field

from open_webui.main import app as webui_app
from open_webui.models.users import Users
from open_webui.models.memories import Memories
from open_webui.routers.memories import (
    add_memory,
    AddMemoryForm,
    query_memory,
    QueryMemoryForm,
    delete_memory_by_id,
)
from open_webui.utils.misc import get_last_assistant_message

# ==================== æç¤ºè¯åº“ ====================
FACT_EXTRACTION_PROMPT = """ä½ æ­£åœ¨å¸®åŠ©ç»´æŠ¤ç”¨æˆ·çš„"è®°å¿†"â€”â€”å°±åƒä¸€ä¸ªä¸ªç‹¬ç«‹çš„"æ—¥è®°æ¡ç›®"ã€‚ä½ å°†æ”¶åˆ°æœ€è¿‘å‡ æ¡å¯¹è¯ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ¤æ–­ç”¨æˆ·çš„ã€æœ€æ–°ä¸€æ¡ã€‘æ¶ˆæ¯ä¸­ï¼Œæœ‰å“ªäº›ç»†èŠ‚å€¼å¾—ä½œä¸º"è®°å¿†"è¢«é•¿æœŸä¿å­˜ã€‚ã€æ ¸å¿ƒæŒ‡ä»¤ã€‘1.  **åªåˆ†æç”¨æˆ·æœ€æ–°ä¸€æ¡æ¶ˆæ¯**ï¼šä»…ä»ç”¨æˆ·çš„æœ€æ–°å‘è¨€ä¸­è¯†åˆ«æ–°çš„æˆ–å˜æ›´çš„ä¸ªäººä¿¡æ¯ã€‚æ—§æ¶ˆæ¯ä»…ä¾›ç†è§£ä¸Šä¸‹æ–‡ã€‚2.  **å¤„ç†ä¿¡æ¯å˜æ›´**ï¼šå¦‚æœç”¨æˆ·æœ€æ–°æ¶ˆæ¯ä¸æ—§ä¿¡æ¯å†²çªï¼Œåªæå–æ›´æ–°åçš„ä¿¡æ¯ã€‚3.  **äº‹å®ç‹¬ç«‹**ï¼šæ¯æ¡è®°å¿†éƒ½åº”æ˜¯ç‹¬ç«‹çš„"äº‹å®"ã€‚å¦‚æœä¸€å¥è¯åŒ…å«å¤šä¸ªä¿¡æ¯ç‚¹ï¼Œè¯·æ‹†åˆ†ã€‚4.  **æå–æœ‰ä»·å€¼ä¿¡æ¯**ï¼šç›®æ ‡æ˜¯æ•æ‰ä»»ä½•æœ‰åŠ©äºAIåœ¨æœªæ¥æä¾›æ›´ä¸ªæ€§åŒ–æœåŠ¡çš„ä¿¡æ¯ã€‚5.  **å“åº”æ˜ç¡®æŒ‡ä»¤**ï¼šå¦‚æœç”¨æˆ·æ˜ç¡®è¦æ±‚"è®°ä½"ï¼Œå¿…é¡»æå–è¯¥ä¿¡æ¯ã€‚6.  **å¿½ç•¥çŸ­æœŸä¿¡æ¯**ï¼šä¸è¦è®°å½•ä¸´æ—¶æˆ–æ— æ„ä¹‰çš„ç»†èŠ‚ã€‚7.  **æŒ‡å®šæ ¼å¼è¿”å›**ï¼šå°†ç»“æœä»¥ã€JSONå­—ç¬¦ä¸²æ•°ç»„ã€‘çš„æ ¼å¼è¿”å›ã€‚å¦‚æ— ä¿¡æ¯ï¼Œã€åªã€‘è¿”å›ç©ºæ•°ç»„ï¼ˆ`[]`ï¼‰ã€‚ä¸è¦åŠ ä»»ä½•è§£é‡Šã€‚---### ã€ç¤ºä¾‹ã€‘**ç¤ºä¾‹ 1**_è¾“å…¥å¯¹è¯:_- user: ```æˆ‘çˆ±åƒæ©˜å­```- assistant: ```å¤ªæ£’äº†ï¼```- user: ```å…¶å®æˆ‘è®¨åŒåƒæ©˜å­```_æ­£ç¡®è¾“å‡º:_["ç”¨æˆ·è®¨åŒåƒæ©˜å­"]**ç¤ºä¾‹ 2**_è¾“å…¥å¯¹è¯:_- user: ```æˆ‘æ˜¯ä¸€ååˆçº§æ•°æ®åˆ†æå¸ˆã€‚è¯·è®°ä½æˆ‘çš„é‡è¦æ±‡æŠ¥åœ¨3æœˆ15æ—¥ã€‚```_æ­£ç¡®è¾“å‡º:_["ç”¨æˆ·æ˜¯ä¸€ååˆçº§æ•°æ®åˆ†æå¸ˆ", "ç”¨æˆ·åœ¨3æœˆ15æ—¥æœ‰ä¸€æ¬¡é‡è¦æ±‡æŠ¥"]"""
SUMMARY_CREATION_PROMPT = """ä½ æ˜¯AIåŠ©æ‰‹ï¼Œä»»åŠ¡æ˜¯ä¸ºä¸€æ®µå¯¹è¯ç”Ÿæˆç®€æ´çš„æ‘˜è¦ï¼Œä½œä¸ºé•¿æœŸè®°å¿†ã€‚ã€æŒ‡ä»¤ã€‘1. **è§†è§’**: ä»¥ç¬¬ä¸€äººç§°ï¼ˆ"æˆ‘"æˆ–"æˆ‘ä»¬"ï¼‰æ€»ç»“ã€‚2. **å†…å®¹**: æ•æ‰æ ¸å¿ƒè¦ç‚¹ï¼šç”¨æˆ·æ„å›¾ã€ä½ çš„å…³é”®å›ç­”ã€åŒæ–¹å…±è¯†ã€‚3. **æ ¼å¼**: **å¿…é¡»**æ˜¯å•ä¸€æ®µè½çš„æ–‡æœ¬ã€‚4. **ç›®æ ‡**: å¦‚æœå¯¹è¯æœ‰å®è´¨å†…å®¹ï¼Œåˆ™åˆ›å»ºè®°å¿†ã€‚å¦åˆ™ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ã€‚"""
SUMMARY_UPDATE_PROMPT = """ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æœ€æ–°çš„å¯¹è¯å†…å®¹ï¼Œæ›´æ–°ä¸€æ®µå·²æœ‰çš„è®°å¿†æ‘˜è¦ã€‚ã€æŒ‡ä»¤ã€‘1. ä½ å°†æ”¶åˆ°ã€æ—§çš„æ‘˜è¦ã€‘å’Œã€æ–°çš„å¯¹è¯å†…å®¹ã€‘ã€‚2. å°†æ–°å¯¹è¯ä¸­çš„ã€æ ¸å¿ƒä¿¡æ¯ã€‘æ— ç¼åœ°èå…¥åˆ°æ—§æ‘˜è¦ä¸­ã€‚3. ä¿æŒç¬¬ä¸€äººç§°è§†è§’ã€‚4. **å¿…é¡»**è¿”å›ä¸€ä¸ªæ›´æ–°åçš„ã€å®Œæ•´çš„ã€å•ä¸€æ®µè½çš„æ‘˜è¦æ–‡æœ¬ã€‚ã€æ—§çš„æ‘˜è¦ã€‘:{existing_summary}ã€æ–°çš„å¯¹è¯å†…å®¹ã€‘:{new_conversation}"""
FACT_CONSOLIDATION_PROMPT = """ä½ æ­£åœ¨ç®¡ç†ç”¨æˆ·çš„"è®°å¿†"ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ¸…ç†ä¸€ä¸ªå¯èƒ½åŒ…å«ç›¸å…³ã€é‡å æˆ–å†²çªä¿¡æ¯çš„è®°å¿†åˆ—è¡¨ã€‚**ã€å¤„ç†è§„åˆ™ã€‘**1. ä½ å°†æ”¶åˆ°ä¸€ä¸ªJSONæ ¼å¼çš„è®°å¿†åˆ—è¡¨ï¼Œæ¯æ¡å«"fact"å’Œ"created_at"æ—¶é—´æˆ³ã€‚2. ç”Ÿæˆä¸€ä¸ªæ¸…ç†åçš„æœ€ç»ˆäº‹å®åˆ—è¡¨ï¼Œç¡®ä¿ï¼š - åªåœ¨è®°å¿†**å®Œå…¨é‡å¤**æˆ–å°±åŒä¸€ä¸»é¢˜**ç›´æ¥å†²çª**æ—¶æ‰åˆå¹¶ã€‚ - å¦‚æœé‡å¤æˆ–å†²çªï¼Œ**åªä¿ç•™`created_at`æœ€æ–°çš„é‚£ä¸€æ¡**ã€‚ - å¦‚æœéƒ¨åˆ†ç›¸ä¼¼ä½†ä¸å†²çªï¼ˆä¾‹å¦‚"ç”¨æˆ·å–œæ¬¢æ©˜å­"å’Œ"ç”¨æˆ·å–œæ¬¢ç†Ÿé€çš„æ©˜å­"ï¼‰ï¼Œåˆ™**ä¸¤è€…éƒ½ä¿ç•™**ã€‚3. è¿”å›æœ€ç»ˆç»“æœæ—¶ï¼Œä½¿ç”¨ä¸€ä¸ªç®€å•çš„ã€JSONå­—ç¬¦ä¸²æ•°ç»„ã€‘æ ¼å¼ã€‚ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€‚---### ã€ç¤ºä¾‹ã€‘_è¾“å…¥ (JSONæ•°ç»„):_`[{"fact": "ç”¨æˆ·æœ€å–œæ¬¢çš„é¢œè‰²æ˜¯é’è‰²", "created_at": 1635500000}, {"fact": "ç”¨æˆ·æœ€å–œæ¬¢çš„é¢œè‰²æ˜¯çº¢è‰²", "created_at": 1636000000}]`_æ­£ç¡®è¾“å‡º (JSONå­—ç¬¦ä¸²æ•°ç»„):_["ç”¨æˆ·æœ€å–œæ¬¢çš„é¢œè‰²æ˜¯çº¢è‰²"]"""

# æ–°å¢ï¼šç”¨äºæ€»ç»“è¶…é¾„è®°å¿†çš„æç¤ºè¯
OLD_MEMORY_SUMMARIZATION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªè®°å¿†æ•´ç†åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ä¸€ç»„è¾ƒæ—§çš„è®°å¿†æ•´ç†æˆä¸€ä¸ªéå¸¸ç®€çŸ­çš„ç»¼åˆæ€§æ‘˜è¦ã€‚

**ã€æ ¸å¿ƒæŒ‡ä»¤ã€‘**
1. **æ—¶é—´èŒƒå›´**ï¼šè¿™äº›è®°å¿†éƒ½æ˜¯è¾ƒæ—©æœŸçš„ï¼Œéœ€è¦æ•´åˆæˆä¸€ä¸ªå†å²æ¦‚è§ˆ
2. **ä¿ç•™è¦ç‚¹**ï¼šä¿ç•™æ‰€æœ‰é‡è¦çš„å†å²ä¿¡æ¯ã€åå¥½å˜åŒ–ã€å…³é”®äº‹ä»¶
3. **æ—¶é—´çº¿ç´¢**ï¼šå¦‚æœæœ‰æ˜æ˜¾çš„æ—¶é—´é¡ºåºæˆ–å˜åŒ–è¶‹åŠ¿ï¼Œè¯·åœ¨æ‘˜è¦ä¸­ä½“ç°
4. **æ ¼å¼è¦æ±‚**ï¼šè¿”å›ä¸€ä¸ªè‡ªç„¶æµç•…çš„æ®µè½ï¼Œæ¦‚æ‹¬è¿™æ®µæ—¶æœŸçš„ç”¨æˆ·ä¿¡æ¯

ç°åœ¨ï¼Œè¯·åˆ†æä»¥ä¸‹çš„å†å²è®°å¿†ï¼Œå¹¶æä¾›ä¸€ä¸ªç»¼åˆæ€§æ‘˜è¦ï¼š"""


# ==================== ä¸»ç±»å®šä¹‰ ====================
class Filter:
    _embedding_model = None

    @classmethod
    def get_embedding_model(cls):
        if cls._embedding_model is None:
            try:
                print("Attempting to load SentenceTransformer model...")
                from sentence_transformers import SentenceTransformer

                cls._embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
                print("Embedding model loaded successfully.")
            except ImportError:
                print(
                    "ERROR: sentence-transformers or numpy not installed. Please run 'pip install sentence-transformers numpy'"
                )
                return None
            except Exception as e:
                print(f"Error loading embedding model: {e}")
                return None
        return cls._embedding_model

    class Valves(BaseModel):
        # åŸºç¡€é…ç½®
        enabled: bool = Field(default=True, description="ã€æ€»å¼€å…³ã€‘å¯ç”¨/ç¦ç”¨æœ¬æ’ä»¶")
        memory_mode: str = Field(
            default="fact",
            description="è®°å¿†æ¨¡å¼: 'fact' (æå–äº‹å®) æˆ– 'summary' (å¯¹è¯æ€»ç»“)",
        )
        api_url: str = Field(
            default="https://api.openai.com/v1/chat/completions",
            description="APIç«¯ç‚¹åœ°å€ (å¿…é¡»åŒ…å«/v1/chat/completions)",
        )
        api_key: str = Field(
            default="",
            description="ã€é‡è¦ã€‘APIå¯†é’¥",
        )
        model: str = Field(default="gpt-4.1-nano", description="ç”¨äºè®°å¿†å¤„ç†çš„æ¨¡å‹")
        show_stats: bool = Field(default=True, description="åœ¨çŠ¶æ€æ æ˜¾ç¤ºæ€§èƒ½ä¸è®°å¿†ç»Ÿè®¡")
        messages_to_consider: int = Field(default=6, description="çº³å…¥åˆ†æçš„æ¶ˆæ¯æ•°é‡")
        timezone: str = Field(
            default="Asia/Shanghai", description="ã€æ—¶é—´ä¿®æ­£ã€‘ç”¨äºç”Ÿæˆæ—¶é—´æˆ³çš„æ—¶åŒº"
        )
        # è®°å¿†æ³¨å…¥é…ç½®
        inject_memories_to_prompt: bool = Field(
            default=True, description="ã€è®°å¿†æ³¨å…¥ã€‘æ˜¯å¦å°†ç”¨æˆ·è®°å¿†æ³¨å…¥åˆ°ç³»ç»Ÿæç¤ºè¯ä¸­"
        )
        memory_injection_prepending_text: str = Field(
            default="\n\nä»¥ä¸‹æ˜¯å­˜å‚¨è®°å¿†çš„åˆ—è¡¨ã€‚è¯·å°†è¿™äº›è®°å¿†ä¸­çš„ä¿¡æ¯è§†ä¸ºäº‹å®ï¼Œå¹¶ç”¨å®ƒä»¬æ¥æŒ‡å¯¼ä½ çš„å›ç­”ï¼Œä½†é™¤éä¸å½“å‰å¯¹è¯ç›´æ¥ç›¸å…³ï¼Œå¦åˆ™ä¸è¦æåŠè®°å¿†åº“ä¸­çš„ä¿¡æ¯ã€‚å¦‚æœæä¾›çš„è®°å¿†åº“ä¸ºç©ºï¼Œè¯·ä¸è¦ç¼–é€ è®°å¿†ã€‚è¿™æ˜¯è®°å¿†åˆ—è¡¨ï¼š",
            description="æ³¨å…¥è®°å¿†å‰çš„æç¤ºæ–‡æœ¬",
        )
        show_memory_count_in_status: bool = Field(
            default=False, description="åœ¨çŠ¶æ€æ æ˜¾ç¤ºæ³¨å…¥çš„è®°å¿†æ•°é‡"
        )
        append_on_empty_memories: bool = Field(
            default=False, description="å³ä½¿æ²¡æœ‰è®°å¿†ä¹Ÿæ³¨å…¥ç©ºçš„è®°å¿†åˆ—è¡¨"
        )
        # è¯­ä¹‰æœç´¢é…ç½®
        enable_semantic_search: bool = Field(
            default=False, description="ã€é«˜çº§åŠŸèƒ½/å¯é€‰ã€‘å¯ç”¨è¯­ä¹‰æœç´¢"
        )
        similarity_threshold: float = Field(
            default=0.8, description="è®°å¿†å»é‡/æ›´æ–°çš„ç›¸ä¼¼åº¦é˜ˆå€¼"
        )
        consolidation_threshold: float = Field(
            default=0.75,
            description="ã€è®°å¿†æ•´åˆã€‘æŸ¥æ‰¾ç›¸å…³æ—§è®°å¿†ä»¥è¿›è¡Œæ•´åˆçš„ç›¸ä¼¼åº¦é˜ˆå€¼ (0-1)",
        )
        preserve_min_memory_age_days: int = Field(
            default=3,
            description="ã€è®°å¿†ä¿ç•™æœŸã€‘è¶…è¿‡æ­¤å¤©æ•°çš„è®°å¿†å°†è¢«æ€»ç»“ååˆ é™¤ï¼ˆ0=ç¦ç”¨æ­¤åŠŸèƒ½ï¼‰",
        )

    def __init__(self):
        self.valves = self.Valves()
        self.start_time = None
        self.time_to_first_token = None
        self.first_chunk_received = False

    async def inlet(
        self,
        body: dict,
        __user__: Optional[dict] = None,
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> dict:
        """å¤„ç†è¾“å…¥ï¼Œæ³¨å…¥è®°å¿†åˆ°ç³»ç»Ÿæç¤ºè¯"""
        self.start_time = time.time()
        self.time_to_first_token = None
        self.first_chunk_received = False

        # å¦‚æœå¯ç”¨äº†è®°å¿†æ³¨å…¥åŠŸèƒ½
        if self.valves.inject_memories_to_prompt and __user__:
            user_id = __user__.get("id")
            if user_id:
                # è·å–ç”¨æˆ·è®°å¿†
                user_memories = Memories.get_memories_by_user_id(user_id)
                num_memories = len(user_memories) if user_memories else 0

                # åœ¨çŠ¶æ€æ æ˜¾ç¤ºè®°å¿†æ•°é‡
                if self.valves.show_memory_count_in_status and __event_emitter__:
                    await __event_emitter__(
                        {
                            "type": "status",
                            "data": {
                                "description": f"åŠ è½½äº† {num_memories} æ¡è®°å¿†",
                                "status": "complete",
                                "done": True,
                            },
                        }
                    )

                # æ„å»ºè®°å¿†åˆ—è¡¨
                memory_list = []

                if user_memories:
                    for memory in user_memories:
                        # è§£æè®°å¿†å†…å®¹ï¼Œå»é™¤æ—¶é—´æˆ³
                        content_without_timestamp, _ = self._parse_memory_content(
                            memory.content
                        )

                        updated_at = memory.updated_at
                        if isinstance(updated_at, int):
                            updated_at = datetime.datetime.fromtimestamp(updated_at)

                        updated_at_str = updated_at.isoformat() if updated_at else None

                        memory_list.append(
                            {
                                "content": content_without_timestamp,
                                "updated_at": updated_at_str,
                            }
                        )

                # å¦‚æœæœ‰è®°å¿†æˆ–è€…è®¾ç½®äº†å³ä½¿ç©ºä¹Ÿè¦æ³¨å…¥
                if memory_list or self.valves.append_on_empty_memories:
                    json_data = json.dumps(memory_list, ensure_ascii=False)
                    system_message = (
                        f"{self.valves.memory_injection_prepending_text}\n{json_data}"
                    )

                    # æŸ¥æ‰¾å¹¶æ›´æ–°ç³»ç»Ÿæ¶ˆæ¯
                    system_msg_found = False
                    for message in body["messages"]:
                        if message["role"] == "system":
                            message["content"] += f"\n{system_message}"
                            system_msg_found = True
                            break

                    # å¦‚æœæ²¡æœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œåˆ›å»ºä¸€ä¸ª
                    if not system_msg_found:
                        body["messages"].insert(
                            0, {"role": "system", "content": system_message}
                        )

                    print(f"Injected {num_memories} memories into system prompt")

        return body

    def stream(self, event: dict) -> dict:
        """å¤„ç†æµå¼å“åº”ï¼Œè®°å½•é¦–å­—æ—¶é—´"""
        if not self.first_chunk_received:
            self.time_to_first_token = time.time() - self.start_time
            self.first_chunk_received = True
        return event

    async def outlet(
        self,
        body: dict,
        __event_emitter__: Callable[[Any], Awaitable[None]],
        __user__: Optional[dict] = None,
    ):
        """å¤„ç†è¾“å‡ºï¼Œæ‰§è¡Œè®°å¿†å­˜å‚¨å’Œç»Ÿè®¡"""
        if not self.valves.enabled or not __user__ or len(body.get("messages", [])) < 2:
            return body

        user = Users.get_user_by_id(__user__["id"])

        # å¤„ç†è¶…é¾„è®°å¿†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.valves.preserve_min_memory_age_days > 0:
            await self._process_old_memories(user)

        try:
            if self.valves.memory_mode == "fact":
                memory_result = await self._handle_fact_mode(body, user)
            elif self.valves.memory_mode == "summary":
                memory_result = await self._handle_summary_mode(body, user)
            else:
                memory_result = {"status": "error", "message": "æ— æ•ˆçš„è®°å¿†æ¨¡å¼"}
        except Exception as e:
            print(f"CRITICAL ERROR in outlet: {e}")
            import traceback

            traceback.print_exc()
            memory_result = {"status": "error", "message": "æ’ä»¶æ‰§è¡Œå‡ºé”™"}

        stats_result = self._calculate_stats(body)

        if self.valves.show_stats:
            await self._show_status(__event_emitter__, memory_result, stats_result)

        return body

    async def _process_old_memories(self, user):
        """å¤„ç†è¶…é¾„è®°å¿†ï¼šæ€»ç»“å¹¶åˆ é™¤"""
        try:
            # è·å–æ‰€æœ‰ç”¨æˆ·è®°å¿†
            all_memories = Memories.get_memories_by_user_id(user.id)
            if not all_memories:
                return

            # è®¡ç®—æ—¶é—´é˜ˆå€¼
            min_age_seconds = self.valves.preserve_min_memory_age_days * 24 * 3600
            now_ts = time.time()

            # ç­›é€‰è¶…é¾„è®°å¿†
            old_memories = []
            for mem in all_memories:
                content, timestamp = self._parse_memory_content(mem.content)
                if timestamp > 0 and (now_ts - timestamp) > min_age_seconds:
                    old_memories.append(
                        {
                            "id": mem.id,
                            "content": mem.content,
                            "content_without_timestamp": content,
                            "timestamp": timestamp,
                        }
                    )

            if not old_memories:
                return

            print(
                f"Found {len(old_memories)} old memories to process for user {user.id}"
            )

            # æ„å»ºè®°å¿†å†…å®¹åˆ—è¡¨
            memory_contents = [f"- {m['content']}" for m in old_memories]
            combined_content = "\n".join(memory_contents)

            # è°ƒç”¨LLMç”Ÿæˆæ€»ç»“
            summary = await self._call_llm(
                OLD_MEMORY_SUMMARIZATION_PROMPT, combined_content
            )

            if summary:
                # å­˜å‚¨æ€»ç»“ä½œä¸ºæ–°è®°å¿†
                summary_with_note = (
                    f"[å†å²è®°å¿†æ€»ç»“ - åŒ…å«{len(old_memories)}æ¡è®°å½•] {summary}"
                )
                await self._store_new_memory(summary_with_note, user, is_summary=True)

                # åˆ é™¤æ—§è®°å¿†
                for mem in old_memories:
                    await delete_memory_by_id(mem["id"], user)

                print(
                    f"Successfully summarized and removed {len(old_memories)} old memories for user {user.id}"
                )

        except Exception as e:
            print(f"Error processing old memories: {e}")

    async def _handle_fact_mode(self, body: dict, user) -> dict:
        conversation_text = self._stringify_conversation(body["messages"])
        if not conversation_text:
            return {"status": "skipped", "message": "æ— æ¶ˆæ¯"}
        try:
            new_facts = await self._call_llm_for_json(
                FACT_EXTRACTION_PROMPT, conversation_text
            )
            if not new_facts:
                return {"status": "success", "message": "æ— æ–°äº‹å®"}
            facts_to_consolidate = []
            related_memories_to_delete_ids = set()
            for fact in new_facts:
                related_memories = await self._query_memories_by_similarity(
                    fact, user, k=5, threshold=self.valves.consolidation_threshold
                )
                consolidation_input = [{"fact": fact, "created_at": time.time()}]
                for mem in related_memories:
                    related_memories_to_delete_ids.add(mem["id"])
                    content, ts = self._parse_memory_content(mem["content"])
                    consolidation_input.append({"fact": content, "created_at": ts})
                facts_to_consolidate.append(consolidation_input)
            final_facts_to_save = []
            for fact_group in facts_to_consolidate:
                prompt_input_json = json.dumps(fact_group, ensure_ascii=False)
                cleaned_facts = await self._call_llm_for_json(
                    FACT_CONSOLIDATION_PROMPT, prompt_input_json
                )
                final_facts_to_save.extend(cleaned_facts)
            if related_memories_to_delete_ids:
                print(
                    f"Consolidating memories, deleting {len(related_memories_to_delete_ids)} old facts..."
                )
                for mem_id in related_memories_to_delete_ids:
                    await delete_memory_by_id(mem_id, user)
            saved_count = 0
            for fact in list(set(final_facts_to_save)):
                await self._store_new_memory(fact, user, is_summary=True)
                saved_count += 1
            return {
                "status": "success",
                "message": f"è®°å¿†æ•´åˆ: æ–°å¢{saved_count}æ¡, æ¸…ç†{len(related_memories_to_delete_ids)}æ¡",
            }
        except Exception as e:
            import traceback

            traceback.print_exc()
            return {"status": "error", "message": f"äº‹å®æ•´åˆå¤±è´¥: {e}"}

    async def _handle_summary_mode(self, body: dict, user) -> dict:
        conversation_text = self._stringify_conversation(body["messages"])
        if not conversation_text:
            return {"status": "skipped", "message": "æ— æ¶ˆæ¯"}
        try:
            related_summary = await self._find_related_summary(conversation_text, user)
            if related_summary:
                updated_summary_content = await self._call_llm(
                    SUMMARY_UPDATE_PROMPT.format(
                        existing_summary=related_summary["content_without_timestamp"],
                        new_conversation=conversation_text,
                    )
                )
                if not updated_summary_content:
                    return {"status": "skipped", "message": "æ›´æ–°æ— å†…å®¹"}
                await delete_memory_by_id(related_summary["id"], user)
                content_with_timestamp = self._add_timestamp_to_content(
                    updated_summary_content
                )
                await add_memory(
                    request=self._get_dummy_request(),
                    form_data=AddMemoryForm(content=content_with_timestamp),
                    user=user,
                )
                return {"status": "success", "message": "æ€»ç»“å·²æ›´æ–°"}
            else:
                new_summary_content = await self._call_llm(
                    SUMMARY_CREATION_PROMPT, conversation_text
                )
                if not new_summary_content:
                    return {"status": "success", "message": "æ— éœ€æ€»ç»“"}
                await self._store_new_memory(new_summary_content, user, is_summary=True)
                return {"status": "success", "message": "å·²åˆ›å»ºæ–°æ€»ç»“"}
        except Exception as e:
            return {"status": "error", "message": f"æ€»ç»“å¤„ç†å¤±è´¥: {e}"}

    def _add_timestamp_to_content(self, content: str) -> str:
        try:
            target_tz = pytz.timezone(self.valves.timezone)
        except pytz.UnknownTimeZoneError:
            target_tz = pytz.utc
        now = datetime.datetime.now(target_tz)
        return f"{now.strftime('%Yå¹´%mæœˆ%dæ—¥%Hç‚¹%Måˆ†')}ï¼š{content}"

    async def _store_new_memory(
        self, content: str, user, is_summary: bool = False
    ) -> bool:
        if not is_summary:
            is_duplicate = False
            if self.valves.enable_semantic_search:
                related_memories = await self._query_memories_by_similarity(
                    content, user, k=1, threshold=self.valves.similarity_threshold
                )
                if related_memories:
                    is_duplicate = True
            else:
                related_memories = await self._query_memories_legacy(content, user, k=1)
                if (
                    related_memories
                    and related_memories[0]["similarity"]
                    > self.valves.similarity_threshold
                ):
                    is_duplicate = True
            if is_duplicate:
                return False
        content_with_timestamp = self._add_timestamp_to_content(content)
        await add_memory(
            request=self._get_dummy_request(),
            form_data=AddMemoryForm(content=content_with_timestamp),
            user=user,
        )
        return True

    def _parse_memory_content(self, content: str) -> tuple:
        """è§£æè®°å¿†å†…å®¹ï¼Œè¿”å›(å†…å®¹, æ—¶é—´æˆ³)"""
        match = re.match(r"^(\d{4}å¹´\d{2}æœˆ\d{2}æ—¥\d{2}ç‚¹\d{2}åˆ†)ï¼š(.*)", content)
        if match:
            try:
                dt_obj = datetime.datetime.strptime(
                    match.group(1), "%Yå¹´%mæœˆ%dæ—¥%Hç‚¹%Måˆ†"
                )
                # åº”ç”¨æ—¶åŒº
                try:
                    target_tz = pytz.timezone(self.valves.timezone)
                except pytz.UnknownTimeZoneError:
                    target_tz = pytz.utc
                dt_obj = target_tz.localize(dt_obj)
                return match.group(2), dt_obj.timestamp()
            except ValueError:
                return match.group(2), 0
        return content, 0

    async def _find_related_summary(self, text: str, user) -> Optional[dict]:
        related_memories = await self._query_memories_by_similarity(
            text, user, k=1, threshold=self.valves.similarity_threshold
        )
        if not related_memories:
            return None
        return related_memories[0]

    async def _query_memories_legacy(self, text: str, user, k: int = 1) -> List[dict]:
        query_result = await query_memory(
            self._get_dummy_request(), QueryMemoryForm(content=text, k=k), user
        )
        if (
            not query_result
            or not hasattr(query_result, "documents")
            or not query_result.documents[0]
        ):
            return []
        results = []
        for i, doc_content in enumerate(query_result.documents[0]):
            content_without_timestamp, _ = self._parse_memory_content(doc_content)
            results.append(
                {
                    "id": query_result.metadatas[0][i].get("id"),
                    "content": doc_content,
                    "content_without_timestamp": content_without_timestamp,
                    "similarity": 1 - query_result.distances[0][i],
                }
            )
        return results

    async def _query_memories_by_similarity(
        self, text: str, user, k: int, threshold: float
    ) -> List[dict]:
        if self.valves.enable_semantic_search:
            return await self._query_memories_semantic(text, user, k, threshold)
        else:
            results = await self._query_memories_legacy(text, user, k)
            return [res for res in results if res["similarity"] > threshold]

    async def _query_memories_semantic(
        self, text: str, user, k: int = 1, threshold: float = 0.0
    ) -> List[dict]:
        model = self.get_embedding_model()
        if not model:
            return []
        import numpy as np

        all_memories_raw = await self._query_memories_legacy(" ", user, k=1000)
        if not all_memories_raw:
            return []
        query_embedding = model.encode(text)
        results = []
        for mem in all_memories_raw:
            mem_embedding = model.encode(mem["content_without_timestamp"])
            similarity = np.dot(query_embedding, mem_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(mem_embedding)
            )
            if similarity >= threshold:
                mem["similarity"] = float(similarity)
                results.append(mem)
        results.sort(key=lambda x: x["similarity"], reverse=True)
        return results[:k]

    async def _call_llm(self, system_prompt: str, user_prompt: str = "") -> str:
        messages = [{"role": "system", "content": system_prompt}]
        if user_prompt:
            messages.append({"role": "user", "content": user_prompt})
        url = self.valves.api_url
        headers = {
            "Authorization": f"Bearer {self.valves.api_key}",
            "Content-Type": "application/json",
        }
        payload = {"model": self.valves.model, "messages": messages, "temperature": 0.0}
        async with aiohttp.ClientSession() as session:
            async with session.post(url, headers=headers, json=payload) as response:
                response.raise_for_status()
                data = await response.json()
                return data["choices"][0]["message"]["content"].strip()

    async def _call_llm_for_json(
        self, system_prompt: str, user_prompt: str = ""
    ) -> List:
        content = await self._call_llm(system_prompt, user_prompt)
        try:
            if content.startswith("```json"):
                content = content[7:-3].strip()
            result = json.loads(content)
            return result if isinstance(result, list) else []
        except json.JSONDecodeError:
            return []

    def _stringify_conversation(self, messages: List[dict]) -> str:
        count = min(self.valves.messages_to_consider, len(messages))
        return "\n".join(
            [f"- {msg['role']}: {msg['content']}" for msg in messages[-count:]]
        )

    def _calculate_stats(self, body: dict) -> dict:
        elapsed = time.time() - self.start_time
        response_msg = get_last_assistant_message(body.get("messages", [])) or ""
        tokens = len(response_msg) // 3
        tps = tokens / elapsed if elapsed > 0 else 0
        return {
            "elapsed": f"{elapsed:.1f}s",
            "tokens": tokens,
            "tps": f"{tps:.0f}",
            "ttft": (
                f"{self.time_to_first_token:.2f}s"
                if self.time_to_first_token is not None
                else "N/A"
            ),
        }

    async def _show_status(
        self, event_emitter, memory_result: dict, stats_result: dict
    ):
        memory_part = []
        if memory_result:
            status, message = memory_result.get("status", "skipped"), memory_result.get(
                "message", ""
            )
            icon = "âœ…" if status == "success" else "âŒ" if status == "error" else "ğŸ¤·"
            memory_part.append(f" {icon} {message}")

        stats_parts = [
            f"é¦–å­—{stats_result['ttft']}",
            f"ç”¨æ—¶{stats_result['elapsed']}",
            f"âš¡{stats_result['tps']}Tok/s",
            f"çº¦{stats_result['tokens']}T",
        ]

        final_parts = memory_part + stats_parts

        if final_parts:
            await event_emitter(
                {
                    "type": "status",
                    "data": {"description": " | ".join(final_parts), "done": True},
                }
            )

    def _get_dummy_request(self) -> Request:
        return Request(scope={"type": "http", "app": webui_app})
